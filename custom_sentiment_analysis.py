# -*- coding: utf-8 -*-
"""Custom_Sentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p5ZmR6AcA2mSh4x_PjmMboHWknzzCRyQ
"""

import pandas as pd
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
import torch


df = pd.read_csv('C:/Users/DASRU NAYAK/Downloads/SMSSpamCollection.txt', sep='\t', names=['label', 'message'])

df.head()

print(df.columns)

df.shape

df.value_counts('label')

df.value_counts('message')

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.describe()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(6, 4))
sns.countplot(x='label', hue='label', data=df, palette='Set2', legend=False)
plt.title('Spam vs Ham Message Distribution')
plt.xlabel('Message Type')
plt.ylabel('Count')
plt.savefig("spam_ham_plot.png")


df['labels']=df['label'].apply(lambda x: 1 if x=='spam' else 0)

df

X=list(df['message'])

y=list(df['labels'])

y

"""Train test Split"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

X_train

from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

train_encodings = tokenizer(list(X_train), truncation=True, padding=True)
test_encodings = tokenizer((X_test), truncation=True, padding=True)

"""converting to Hugging Face Dataset"""

import torch
train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),
                                               torch.tensor(train_encodings['attention_mask']),
                                               torch.tensor(y_train))

class SentientDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = SentientDataset(train_encodings, list(y_train))
test_dataset = SentientDataset(test_encodings, list(y_test))

"""Load Pretrained BERT for Classification"""

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

"""Fine-Tune The Model"""

import os
os.environ["WANDB_DISABLED"] = "true"

training_args=TrainingArguments(output_dir='./results',
                                num_train_epochs=2,
                                learning_rate=2e-5,
                                per_device_train_batch_size=8,
                                per_device_eval_batch_size=8,
                                weight_decay=0.01,
                                logging_dir='./logs',
                                logging_steps=10,)

trainer=Trainer(model=model,args=training_args,train_dataset=train_dataset,eval_dataset=test_dataset)

trainer.train()

"""Evaluate & Predict"""

# Evaluate
trainer.evaluate()

"""Deploying Streamlit

"""

import streamlit as st

"""Title Text"""

st.title("Custom_Sentiment_Analysis -Using The Hugging Face BERT")
st.subheader("Enetre the Sentence To Classify as Positive or Negative")

"""Input Text"""

text=st.text_input("Enter the Sentence")

"""Predict Button"""

MODEL_NAME='bert-base-uncased'
tokenizer=BertTokenizer.from_pretrained(MODEL_NAME)
model=BertForSequenceClassification.from_pretrained(MODEL_NAME,num_labels=2)

if st.button("Classify"):
  if not text.strip():
    st.warning("Please some enter a sentence.")
  else:
    # Tokenize input
    inputs=tokenizer(text,return_tensors="pt",truncation=True,Padding=True,max_length=128)

    # Make prediction
    with torch.no_grad():
      outputs=model(**inputs)
      logits=outputs.logits
      prediction=torch.argmax(logits,dim=1).item()

    #Display Result
    if prediction==1:
      st.success("The Sentence is Positive")
    else:
      st.error("The Sentence is Negative")

"""Save the model"""

model.save_pretrained("saved_bert_model")
tokenizer.save_pretrained("saved_bert_model")